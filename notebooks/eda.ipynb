{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0858f8a4",
   "metadata": {},
   "source": [
    "# NASA Space Apps Challenge 2025: Exoplanet Detection EDA\n",
    "\n",
    "## Challenge: A World Away ‚Äì Hunting for Exoplanets with AI\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis on three major NASA exoplanet datasets:\n",
    "- **Kepler Objects of Interest (KOI)**: Discoveries from the Kepler Space Telescope\n",
    "- **K2 Planets and Candidates**: Extended mission observations \n",
    "- **TESS Objects of Interest (TOI)**: Transiting Exoplanet Survey Satellite data\n",
    "\n",
    "Our goal is to build an AI system that can identify and classify potential exoplanets from stellar observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc9516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"üìä Libraries imported successfully!\")\n",
    "print(\"üöÄ Ready for NASA Space Apps Challenge EDA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f5bd7",
   "metadata": {},
   "source": [
    "## 1. Load Datasets\n",
    "\n",
    "Let's load our three NASA exoplanet datasets and examine their basic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf5d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the three NASA exoplanet datasets\n",
    "data_path = Path(\"../data/raw\")\n",
    "\n",
    "try:\n",
    "    # Load Kepler Objects of Interest\n",
    "    kepler_df = pd.read_csv(data_path / \"kepler.csv\", comment='#', low_memory=False)\n",
    "    print(\"‚úÖ Kepler dataset loaded successfully\")\n",
    "    \n",
    "    # Load K2 Planets and Candidates  \n",
    "    k2_df = pd.read_csv(data_path / \"k2.csv\", comment='#', low_memory=False)\n",
    "    print(\"‚úÖ K2 dataset loaded successfully\")\n",
    "    \n",
    "    # Load TESS Objects of Interest\n",
    "    tess_df = pd.read_csv(data_path / \"tess.csv\", comment='#', low_memory=False)\n",
    "    print(\"‚úÖ TESS dataset loaded successfully\")\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Summary:\")\n",
    "    print(f\"   Kepler: {kepler_df.shape[0]:,} objects, {kepler_df.shape[1]} features\")\n",
    "    print(f\"   K2: {k2_df.shape[0]:,} objects, {k2_df.shape[1]} features\")\n",
    "    print(f\"   TESS: {tess_df.shape[0]:,} objects, {tess_df.shape[1]} features\")\n",
    "    print(f\"   Total: {kepler_df.shape[0] + k2_df.shape[0] + tess_df.shape[0]:,} exoplanet candidates\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error loading datasets: {e}\")\n",
    "    print(\"Please ensure the datasets are in '../data/raw/' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a11bd",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview and Shape Analysis\n",
    "\n",
    "Let's examine the structure of each dataset - their columns, data types, and first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd15b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kepler Dataset Overview\n",
    "print(\"üî≠ KEPLER OBJECTS OF INTEREST (KOI)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {kepler_df.shape}\")\n",
    "print(f\"Columns: {list(kepler_df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "kepler_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K2 Dataset Overview  \n",
    "print(\"üõ∞Ô∏è K2 PLANETS AND CANDIDATES\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {k2_df.shape}\")\n",
    "print(f\"Columns: {list(k2_df.columns[:20])}...\")  # Show first 20 columns\n",
    "print(\"\\nFirst few rows:\")\n",
    "k2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bfb758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESS Dataset Overview\n",
    "print(\"üåü TESS OBJECTS OF INTEREST (TOI)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {tess_df.shape}\")\n",
    "print(f\"Columns: {list(tess_df.columns[:20])}...\")  # Show first 20 columns\n",
    "print(\"\\nFirst few rows:\")\n",
    "tess_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb680589",
   "metadata": {},
   "source": [
    "## 3. Missing Values Analysis\n",
    "\n",
    "Understanding the completeness of our data across all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_values(df, dataset_name):\n",
    "    \"\"\"Analyze missing values in a dataset\"\"\"\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_percentages = (missing_counts / len(df)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': missing_counts.index,\n",
    "        'Missing_Count': missing_counts.values,\n",
    "        'Missing_Percentage': missing_percentages.values\n",
    "    }).sort_values('Missing_Percentage', ascending=False)\n",
    "    \n",
    "    # Only show columns with missing values\n",
    "    missing_df = missing_df[missing_df['Missing_Count'] > 0]\n",
    "    \n",
    "    print(f\"\\nüìä {dataset_name} - Missing Values Summary\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    print(f\"Columns with missing values: {len(missing_df)}\")\n",
    "    print(f\"Complete columns: {len(df.columns) - len(missing_df)}\")\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        print(f\"\\nTop 10 columns with most missing values:\")\n",
    "        print(missing_df.head(10).to_string(index=False))\n",
    "    else:\n",
    "        print(\"üéâ No missing values found!\")\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "# Analyze missing values for each dataset\n",
    "kepler_missing = analyze_missing_values(kepler_df, \"KEPLER\")\n",
    "k2_missing = analyze_missing_values(k2_df, \"K2\") \n",
    "tess_missing = analyze_missing_values(tess_df, \"TESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values with heatmaps\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "datasets = [\n",
    "    (kepler_df, \"Kepler Objects of Interest\", axes[0]),\n",
    "    (k2_df, \"K2 Planets and Candidates\", axes[1]), \n",
    "    (tess_df, \"TESS Objects of Interest\", axes[2])\n",
    "]\n",
    "\n",
    "for df, title, ax in datasets:\n",
    "    # Create missing value matrix (sample columns for visibility)\n",
    "    sample_cols = df.select_dtypes(include=[np.number]).columns[:20]  # First 20 numeric columns\n",
    "    missing_matrix = df[sample_cols].isnull()\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(missing_matrix.T, cbar=True, cmap='viridis', ax=ax, \n",
    "                xticklabels=False, yticklabels=True)\n",
    "    ax.set_title(f'{title} - Missing Values Pattern (First 20 Numeric Columns)')\n",
    "    ax.set_xlabel('Samples')\n",
    "    ax.set_ylabel('Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414bc57b",
   "metadata": {},
   "source": [
    "## 4. Class Distribution Analysis\n",
    "\n",
    "Analyzing the target class distributions across all three datasets to understand the balance of confirmed planets, candidates, and false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f2d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target columns for each dataset\n",
    "def find_target_columns(df):\n",
    "    \"\"\"Find potential target columns containing disposition/classification\"\"\"\n",
    "    target_cols = []\n",
    "    for col in df.columns:\n",
    "        if any(keyword in col.lower() for keyword in ['disposition', 'disp', 'status', 'classification']):\n",
    "            target_cols.append(col)\n",
    "    return target_cols\n",
    "\n",
    "# Find target columns\n",
    "kepler_targets = find_target_columns(kepler_df)\n",
    "k2_targets = find_target_columns(k2_df)\n",
    "tess_targets = find_target_columns(tess_df)\n",
    "\n",
    "print(\"üéØ Target Columns Found:\")\n",
    "print(f\"Kepler: {kepler_targets}\")\n",
    "print(f\"K2: {k2_targets}\")  \n",
    "print(f\"TESS: {tess_targets}\")\n",
    "\n",
    "# Analyze class distributions\n",
    "def analyze_class_distribution(df, target_col, dataset_name):\n",
    "    \"\"\"Analyze and visualize class distribution\"\"\"\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"‚ùå Column '{target_col}' not found in {dataset_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Get value counts\n",
    "    class_counts = df[target_col].value_counts()\n",
    "    class_percentages = df[target_col].value_counts(normalize=True) * 100\n",
    "    \n",
    "    print(f\"\\nüìä {dataset_name} - Class Distribution ({target_col}):\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        percentage = class_percentages[class_name]\n",
    "        print(f\"   {class_name}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "# Try to identify the main target columns\n",
    "try:\n",
    "    if 'koi_disposition' in kepler_df.columns:\n",
    "        kepler_classes = analyze_class_distribution(kepler_df, 'koi_disposition', 'KEPLER')\n",
    "    elif len(kepler_targets) > 0:\n",
    "        kepler_classes = analyze_class_distribution(kepler_df, kepler_targets[0], 'KEPLER')\n",
    "    else:\n",
    "        print(\"‚ùå No disposition column found in Kepler dataset\")\n",
    "        kepler_classes = None\n",
    "        \n",
    "    if 'disposition' in k2_df.columns:\n",
    "        k2_classes = analyze_class_distribution(k2_df, 'disposition', 'K2')\n",
    "    elif len(k2_targets) > 0:\n",
    "        k2_classes = analyze_class_distribution(k2_df, k2_targets[0], 'K2')\n",
    "    else:\n",
    "        print(\"‚ùå No disposition column found in K2 dataset\") \n",
    "        k2_classes = None\n",
    "        \n",
    "    if 'tfopwg_disp' in tess_df.columns:\n",
    "        tess_classes = analyze_class_distribution(tess_df, 'tfopwg_disp', 'TESS')\n",
    "    elif len(tess_targets) > 0:\n",
    "        tess_classes = analyze_class_distribution(tess_df, tess_targets[0], 'TESS')\n",
    "    else:\n",
    "        print(\"‚ùå No disposition column found in TESS dataset\")\n",
    "        tess_classes = None\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error analyzing class distributions: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('üéØ Class Distribution Across NASA Exoplanet Datasets', fontsize=16, fontweight='bold')\n",
    "\n",
    "datasets_info = [\n",
    "    (kepler_classes, 'KEPLER', 'Kepler Objects of Interest'),\n",
    "    (k2_classes, 'K2', 'K2 Planets and Candidates'),\n",
    "    (tess_classes, 'TESS', 'TESS Objects of Interest')\n",
    "]\n",
    "\n",
    "for i, (class_data, dataset_name, full_name) in enumerate(datasets_info):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    if class_data is not None and len(class_data) > 0:\n",
    "        # Create pie chart\n",
    "        wedges, texts, autotexts = ax.pie(class_data.values, \n",
    "                                         labels=class_data.index,\n",
    "                                         autopct='%1.1f%%',\n",
    "                                         startangle=90,\n",
    "                                         colors=plt.cm.Set3(range(len(class_data))))\n",
    "        \n",
    "        # Customize the text\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_color('black')\n",
    "            autotext.set_fontweight('bold')\n",
    "            autotext.set_fontsize(10)\n",
    "            \n",
    "        ax.set_title(f'{dataset_name}\\n({full_name})', fontsize=12, fontweight='bold')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f'No class data\\navailable for\\n{dataset_name}', \n",
    "                ha='center', va='center', fontsize=12)\n",
    "        ax.set_title(f'{dataset_name}\\n({full_name})', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nüìã Class Distribution Summary:\")\n",
    "total_confirmed = 0\n",
    "total_candidates = 0\n",
    "total_false_positives = 0\n",
    "\n",
    "for class_data, dataset_name, _ in datasets_info:\n",
    "    if class_data is not None:\n",
    "        # Try to map common class names\n",
    "        for class_name, count in class_data.items():\n",
    "            class_lower = str(class_name).lower()\n",
    "            if 'confirmed' in class_lower or 'planet' in class_lower:\n",
    "                total_confirmed += count\n",
    "            elif 'candidate' in class_lower:\n",
    "                total_candidates += count\n",
    "            elif 'false' in class_lower or 'positive' in class_lower:\n",
    "                total_false_positives += count\n",
    "\n",
    "print(f\"   Estimated Confirmed Planets: {total_confirmed:,}\")\n",
    "print(f\"   Estimated Candidates: {total_candidates:,}\")\n",
    "print(f\"   Estimated False Positives: {total_false_positives:,}\")\n",
    "print(f\"   Total Objects: {total_confirmed + total_candidates + total_false_positives:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fca249",
   "metadata": {},
   "source": [
    "## 5. Numeric Feature Distributions\n",
    "\n",
    "Analyzing the distributions of key astrophysical parameters across all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8351d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify key numeric features\n",
    "def identify_key_features(df, dataset_name):\n",
    "    \"\"\"Identify key astrophysical parameters for exoplanet analysis\"\"\"\n",
    "    key_patterns = [\n",
    "        'period', 'radius', 'temp', 'magnitude', 'depth', 'duration', \n",
    "        'impact', 'flux', 'stellar', 'planet', 'transit', 'depth',\n",
    "        'snr', 'mass', 'density', 'distance', 'brightness'\n",
    "    ]\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    key_features = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        col_lower = col.lower()\n",
    "        if any(pattern in col_lower for pattern in key_patterns):\n",
    "            key_features.append(col)\n",
    "    \n",
    "    print(f\"üîç Key Features for {dataset_name}:\")\n",
    "    for i, feature in enumerate(key_features[:10], 1):  # Show top 10\n",
    "        print(f\"   {i:2d}. {feature}\")\n",
    "    if len(key_features) > 10:\n",
    "        print(f\"   ... and {len(key_features) - 10} more features\")\n",
    "    \n",
    "    return key_features\n",
    "\n",
    "# Find key features for each dataset\n",
    "kepler_features = identify_key_features(kepler_df, 'KEPLER')\n",
    "k2_features = identify_key_features(k2_df, 'K2')\n",
    "tess_features = identify_key_features(tess_df, 'TESS')\n",
    "\n",
    "# Function to plot feature distributions\n",
    "def plot_feature_distributions(df, features, dataset_name, max_features=8):\n",
    "    \"\"\"Plot distributions of key numeric features\"\"\"\n",
    "    features_to_plot = features[:max_features]\n",
    "    \n",
    "    if len(features_to_plot) == 0:\n",
    "        print(f\"‚ùå No features to plot for {dataset_name}\")\n",
    "        return\n",
    "    \n",
    "    # Calculate grid size\n",
    "    n_features = len(features_to_plot)\n",
    "    n_cols = min(4, n_features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "    fig.suptitle(f'üåü {dataset_name} - Key Feature Distributions', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    if n_features == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes if hasattr(axes, '__len__') else [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(features_to_plot):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        try:\n",
    "            # Get non-null values\n",
    "            values = df[feature].dropna()\n",
    "            \n",
    "            if len(values) == 0:\n",
    "                ax.text(0.5, 0.5, f'No valid data\\nfor {feature}', \n",
    "                       ha='center', va='center', fontsize=10)\n",
    "                ax.set_title(f'{feature}\\n(No Data)', fontsize=10)\n",
    "                continue\n",
    "            \n",
    "            # Create histogram\n",
    "            ax.hist(values, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            ax.set_title(f'{feature}\\n(n={len(values):,})', fontsize=10, fontweight='bold')\n",
    "            ax.set_xlabel(feature)\n",
    "            ax.set_ylabel('Frequency')\n",
    "            \n",
    "            # Add statistics text\n",
    "            stats_text = f'Mean: {values.mean():.2e}\\nStd: {values.std():.2e}'\n",
    "            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, \n",
    "                   verticalalignment='top', fontsize=8,\n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            \n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, f'Error plotting\\n{feature}\\n{str(e)}', \n",
    "                   ha='center', va='center', fontsize=8)\n",
    "            ax.set_title(f'{feature}\\n(Error)', fontsize=10)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"üìä Plotting feature distributions for each dataset...\\n\")\n",
    "\n",
    "# Plot distributions for each dataset\n",
    "plot_feature_distributions(kepler_df, kepler_features, 'KEPLER')\n",
    "plot_feature_distributions(k2_df, k2_features, 'K2')  \n",
    "plot_feature_distributions(tess_df, tess_features, 'TESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c4c958",
   "metadata": {},
   "source": [
    "## 6. Feature Correlation Analysis\n",
    "\n",
    "Exploring correlations between key astrophysical parameters to understand feature relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10acfd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create correlation matrix\n",
    "def analyze_correlations(df, features, dataset_name, min_features=5):\n",
    "    \"\"\"Analyze and visualize feature correlations\"\"\"\n",
    "    if len(features) < min_features:\n",
    "        print(f\"‚ùå {dataset_name}: Need at least {min_features} features for correlation analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Select top features with good data coverage\n",
    "    feature_coverage = {}\n",
    "    for feature in features:\n",
    "        if feature in df.columns:\n",
    "            coverage = (1 - df[feature].isnull().mean()) * 100\n",
    "            feature_coverage[feature] = coverage\n",
    "    \n",
    "    # Sort by coverage and select top features\n",
    "    sorted_features = sorted(feature_coverage.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_features = [f[0] for f in sorted_features[:12] if f[1] > 50]  # At least 50% data coverage\n",
    "    \n",
    "    if len(top_features) < 3:\n",
    "        print(f\"‚ùå {dataset_name}: Insufficient features with good data coverage\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üîó {dataset_name} - Correlation Analysis:\")\n",
    "    print(f\"   Selected {len(top_features)} features with >50% data coverage\")\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_data = df[top_features].corr()\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_data, dtype=bool), k=1)\n",
    "    \n",
    "    sns.heatmap(corr_data, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                cmap='RdYlBu_r', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                fmt='.2f',\n",
    "                cbar_kws={'label': 'Correlation Coefficient'})\n",
    "    \n",
    "    plt.title(f'üîó {dataset_name} - Feature Correlation Matrix', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highly correlated pairs\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_data.columns)):\n",
    "        for j in range(i+1, len(corr_data.columns)):\n",
    "            corr_val = corr_data.iloc[i, j]\n",
    "            if abs(corr_val) > 0.7:  # High correlation threshold\n",
    "                high_corr_pairs.append((\n",
    "                    corr_data.columns[i], \n",
    "                    corr_data.columns[j], \n",
    "                    corr_val\n",
    "                ))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(f\"\\n‚ö†Ô∏è  High Correlations (|r| > 0.7):\")\n",
    "        for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "            print(f\"   {feat1} ‚Üî {feat2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No highly correlated feature pairs found (|r| > 0.7)\")\n",
    "    \n",
    "    return corr_data\n",
    "\n",
    "# Analyze correlations for each dataset\n",
    "print(\"üîó Analyzing feature correlations across all datasets...\\n\")\n",
    "\n",
    "kepler_corr = analyze_correlations(kepler_df, kepler_features, 'KEPLER')\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "k2_corr = analyze_correlations(k2_df, k2_features, 'K2')\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "tess_corr = analyze_correlations(tess_df, tess_features, 'TESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e61cab",
   "metadata": {},
   "source": [
    "## 7. Cross-Dataset Comparison\n",
    "\n",
    "Comparing feature availability and data quality across the three NASA datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d40a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-dataset comparison\n",
    "def compare_datasets():\n",
    "    \"\"\"Compare key characteristics across all three datasets\"\"\"\n",
    "    \n",
    "    datasets = {\n",
    "        'KEPLER': kepler_df,\n",
    "        'K2': k2_df, \n",
    "        'TESS': tess_df\n",
    "    }\n",
    "    \n",
    "    features = {\n",
    "        'KEPLER': kepler_features,\n",
    "        'K2': k2_features,\n",
    "        'TESS': tess_features\n",
    "    }\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for name, df in datasets.items():\n",
    "        row = {\n",
    "            'Dataset': name,\n",
    "            'Total Objects': len(df),\n",
    "            'Total Features': len(df.columns),\n",
    "            'Key Features': len(features[name]),\n",
    "            'Memory Usage (MB)': df.memory_usage(deep=True).sum() / 1024 / 1024,\n",
    "            'Missing Data (%)': (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
    "        }\n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"üöÄ NASA Exoplanet Datasets Comparison:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(comparison_df.to_string(index=False, float_format='%.1f'))\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "comparison_summary = compare_datasets()\n",
    "\n",
    "# Feature overlap analysis\n",
    "def analyze_feature_overlap():\n",
    "    \"\"\"Analyze common features across datasets\"\"\"\n",
    "    \n",
    "    # Get column names (lowercase for comparison)\n",
    "    kepler_cols = set(col.lower() for col in kepler_df.columns)\n",
    "    k2_cols = set(col.lower() for col in k2_df.columns)\n",
    "    tess_cols = set(col.lower() for col in tess_df.columns)\n",
    "    \n",
    "    # Find overlaps\n",
    "    all_three = kepler_cols & k2_cols & tess_cols\n",
    "    kepler_k2 = (kepler_cols & k2_cols) - all_three\n",
    "    kepler_tess = (kepler_cols & tess_cols) - all_three\n",
    "    k2_tess = (k2_cols & tess_cols) - all_three\n",
    "    \n",
    "    # Unique to each\n",
    "    kepler_only = kepler_cols - k2_cols - tess_cols\n",
    "    k2_only = k2_cols - kepler_cols - tess_cols\n",
    "    tess_only = tess_cols - kepler_cols - k2_cols\n",
    "    \n",
    "    print(\"\\nüîç Feature Overlap Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìä Common to all three datasets: {len(all_three)}\")\n",
    "    if len(all_three) > 0:\n",
    "        print(f\"   Examples: {list(all_three)[:5]}\")\n",
    "    \n",
    "    print(f\"\\nü§ù Shared between pairs:\")\n",
    "    print(f\"   KEPLER & K2 only: {len(kepler_k2)}\")\n",
    "    print(f\"   KEPLER & TESS only: {len(kepler_tess)}\")\n",
    "    print(f\"   K2 & TESS only: {len(k2_tess)}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Unique to each dataset:\")\n",
    "    print(f\"   KEPLER unique: {len(kepler_only)}\")\n",
    "    print(f\"   K2 unique: {len(k2_only)}\")\n",
    "    print(f\"   TESS unique: {len(tess_only)}\")\n",
    "    \n",
    "    # Create Venn diagram data\n",
    "    overlap_data = {\n",
    "        'All Three': len(all_three),\n",
    "        'KEPLER & K2': len(kepler_k2),\n",
    "        'KEPLER & TESS': len(kepler_tess),\n",
    "        'K2 & TESS': len(k2_tess),\n",
    "        'KEPLER Only': len(kepler_only),\n",
    "        'K2 Only': len(k2_only),\n",
    "        'TESS Only': len(tess_only)\n",
    "    }\n",
    "    \n",
    "    return overlap_data\n",
    "\n",
    "overlap_results = analyze_feature_overlap()\n",
    "\n",
    "# Visualize dataset comparison\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('üåå NASA Exoplanet Datasets - Comparative Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Object counts\n",
    "ax1.bar(comparison_summary['Dataset'], comparison_summary['Total Objects'], \n",
    "        color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "ax1.set_title('Total Objects per Dataset', fontweight='bold')\n",
    "ax1.set_ylabel('Number of Objects')\n",
    "for i, v in enumerate(comparison_summary['Total Objects']):\n",
    "    ax1.text(i, v + max(comparison_summary['Total Objects'])*0.01, f'{v:,}', \n",
    "             ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Feature counts\n",
    "ax2.bar(comparison_summary['Dataset'], comparison_summary['Total Features'], \n",
    "        color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "ax2.set_title('Total Features per Dataset', fontweight='bold')\n",
    "ax2.set_ylabel('Number of Features')\n",
    "for i, v in enumerate(comparison_summary['Total Features']):\n",
    "    ax2.text(i, v + max(comparison_summary['Total Features'])*0.01, str(v), \n",
    "             ha='center', fontweight='bold')\n",
    "\n",
    "# 3. Missing data percentage\n",
    "ax3.bar(comparison_summary['Dataset'], comparison_summary['Missing Data (%)'], \n",
    "        color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "ax3.set_title('Missing Data Percentage', fontweight='bold')\n",
    "ax3.set_ylabel('Missing Data (%)')\n",
    "for i, v in enumerate(comparison_summary['Missing Data (%)']):\n",
    "    ax3.text(i, v + max(comparison_summary['Missing Data (%)'])*0.01, f'{v:.1f}%', \n",
    "             ha='center', fontweight='bold')\n",
    "\n",
    "# 4. Memory usage\n",
    "ax4.bar(comparison_summary['Dataset'], comparison_summary['Memory Usage (MB)'], \n",
    "        color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "ax4.set_title('Memory Usage', fontweight='bold')\n",
    "ax4.set_ylabel('Memory (MB)')\n",
    "for i, v in enumerate(comparison_summary['Memory Usage (MB)']):\n",
    "    ax4.text(i, v + max(comparison_summary['Memory Usage (MB)'])*0.01, f'{v:.1f}', \n",
    "             ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b234f1",
   "metadata": {},
   "source": [
    "## 8. Data Quality Assessment\n",
    "\n",
    "Comprehensive evaluation of data quality issues and recommendations for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c1cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality assessment\n",
    "def assess_data_quality(df, dataset_name):\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    \n",
    "    print(f\"üîç Data Quality Assessment for {dataset_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    quality_report = {}\n",
    "    \n",
    "    # 1. Basic statistics\n",
    "    total_rows = len(df)\n",
    "    total_cols = len(df.columns)\n",
    "    total_cells = total_rows * total_cols\n",
    "    \n",
    "    print(f\"üìä Dataset Overview:\")\n",
    "    print(f\"   ‚Ä¢ Total rows: {total_rows:,}\")\n",
    "    print(f\"   ‚Ä¢ Total columns: {total_cols:,}\")\n",
    "    print(f\"   ‚Ä¢ Total cells: {total_cells:,}\")\n",
    "    \n",
    "    # 2. Missing data analysis\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_percentages = (missing_counts / total_rows) * 100\n",
    "    \n",
    "    # Categorize columns by missing data\n",
    "    complete_cols = missing_counts[missing_counts == 0]\n",
    "    low_missing = missing_counts[(missing_counts > 0) & (missing_percentages <= 10)]\n",
    "    medium_missing = missing_counts[(missing_percentages > 10) & (missing_percentages <= 50)]\n",
    "    high_missing = missing_counts[missing_percentages > 50]\n",
    "    \n",
    "    print(f\"\\nüö´ Missing Data Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Complete columns (0% missing): {len(complete_cols)}\")\n",
    "    print(f\"   ‚Ä¢ Low missing (1-10%): {len(low_missing)}\")\n",
    "    print(f\"   ‚Ä¢ Medium missing (11-50%): {len(medium_missing)}\")\n",
    "    print(f\"   ‚Ä¢ High missing (>50%): {len(high_missing)}\")\n",
    "    \n",
    "    if len(high_missing) > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  High missing columns: {list(high_missing.index[:5])}\")\n",
    "    \n",
    "    # 3. Data type analysis\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    object_cols = df.select_dtypes(include=['object']).columns\n",
    "    datetime_cols = df.select_dtypes(include=['datetime64']).columns\n",
    "    \n",
    "    print(f\"\\nüìã Data Types:\")\n",
    "    print(f\"   ‚Ä¢ Numeric columns: {len(numeric_cols)}\")\n",
    "    print(f\"   ‚Ä¢ Text/Object columns: {len(object_cols)}\")\n",
    "    print(f\"   ‚Ä¢ Datetime columns: {len(datetime_cols)}\")\n",
    "    \n",
    "    # 4. Duplicate analysis\n",
    "    duplicate_rows = df.duplicated().sum()\n",
    "    duplicate_percentage = (duplicate_rows / total_rows) * 100\n",
    "    \n",
    "    print(f\"\\nüîÑ Duplicate Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Duplicate rows: {duplicate_rows:,} ({duplicate_percentage:.2f}%)\")\n",
    "    \n",
    "    # 5. Outlier detection for numeric columns (basic)\n",
    "    outlier_summary = {}\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nüìà Outlier Analysis (IQR method):\")\n",
    "        outlier_count = 0\n",
    "        for col in numeric_cols[:10]:  # Check first 10 numeric columns\n",
    "            if df[col].notna().sum() > 0:\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "                outlier_pct = (outliers / df[col].notna().sum()) * 100\n",
    "                \n",
    "                if outliers > 0 and outlier_count < 5:  # Show top 5\n",
    "                    print(f\"   ‚Ä¢ {col}: {outliers:,} outliers ({outlier_pct:.1f}%)\")\n",
    "                    outlier_count += 1\n",
    "                \n",
    "                outlier_summary[col] = outliers\n",
    "    \n",
    "    # 6. Value consistency check\n",
    "    print(f\"\\n‚úÖ Data Consistency:\")\n",
    "    consistency_issues = 0\n",
    "    \n",
    "    # Check for mixed case in text columns\n",
    "    for col in object_cols[:5]:  # Check first 5 object columns\n",
    "        unique_vals = df[col].dropna().astype(str)\n",
    "        if len(unique_vals) > 0:\n",
    "            # Check for potential case sensitivity issues\n",
    "            unique_lower = unique_vals.str.lower().nunique()\n",
    "            unique_original = unique_vals.nunique()\n",
    "            \n",
    "            if unique_lower < unique_original:\n",
    "                consistency_issues += 1\n",
    "                print(f\"   ‚ö†Ô∏è  {col}: Potential case sensitivity issues\")\n",
    "    \n",
    "    if consistency_issues == 0:\n",
    "        print(f\"   ‚úÖ No obvious consistency issues detected\")\n",
    "    \n",
    "    # 7. Summary recommendations\n",
    "    print(f\"\\nüí° Preprocessing Recommendations:\")\n",
    "    \n",
    "    if len(high_missing) > 0:\n",
    "        print(f\"   ‚Ä¢ Consider removing columns with >50% missing data ({len(high_missing)} columns)\")\n",
    "    \n",
    "    if duplicate_rows > 0:\n",
    "        print(f\"   ‚Ä¢ Remove {duplicate_rows:,} duplicate rows\")\n",
    "    \n",
    "    if len(medium_missing) > 0:\n",
    "        print(f\"   ‚Ä¢ Apply imputation strategies for {len(medium_missing)} columns with 11-50% missing data\")\n",
    "    \n",
    "    if any(outlier_summary.values()):\n",
    "        high_outlier_cols = sum(1 for v in outlier_summary.values() if v > total_rows * 0.05)\n",
    "        if high_outlier_cols > 0:\n",
    "            print(f\"   ‚Ä¢ Review outliers in {high_outlier_cols} columns (>5% outliers)\")\n",
    "    \n",
    "    if consistency_issues > 0:\n",
    "        print(f\"   ‚Ä¢ Standardize text values for consistency\")\n",
    "    \n",
    "    quality_score = 100 - (missing_percentages.mean() + duplicate_percentage + consistency_issues * 5)\n",
    "    quality_score = max(0, min(100, quality_score))  # Clamp between 0-100\n",
    "    \n",
    "    print(f\"\\n‚≠ê Overall Data Quality Score: {quality_score:.1f}/100\")\n",
    "    \n",
    "    return {\n",
    "        'dataset': dataset_name,\n",
    "        'total_rows': total_rows,\n",
    "        'total_cols': total_cols,\n",
    "        'complete_cols': len(complete_cols),\n",
    "        'high_missing_cols': len(high_missing),\n",
    "        'duplicate_rows': duplicate_rows,\n",
    "        'consistency_issues': consistency_issues,\n",
    "        'quality_score': quality_score\n",
    "    }\n",
    "\n",
    "# Assess data quality for all datasets\n",
    "print(\"üîç Comprehensive Data Quality Assessment\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "kepler_quality = assess_data_quality(kepler_df, 'KEPLER')\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "k2_quality = assess_data_quality(k2_df, 'K2')\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "tess_quality = assess_data_quality(tess_df, 'TESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc61831",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "Key findings from the exploratory data analysis and recommendations for the NASA Space Apps Challenge solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d170b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "print(\"üåü NASA Space Apps Challenge 2025 - EDA Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Challenge: A World Away: Hunting for Exoplanets with AI\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Combine quality assessments\n",
    "quality_summary = pd.DataFrame([kepler_quality, k2_quality, tess_quality])\n",
    "\n",
    "print(\"\\nüìä DATASET OVERVIEW:\")\n",
    "print(quality_summary[['dataset', 'total_rows', 'total_cols', 'quality_score']].to_string(index=False))\n",
    "\n",
    "print(f\"\\nüéØ KEY FINDINGS:\")\n",
    "print(f\"   ‚Ä¢ Total exoplanet objects across all datasets: {quality_summary['total_rows'].sum():,}\")\n",
    "print(f\"   ‚Ä¢ Combined feature space: {quality_summary['total_cols'].sum()} total columns\")\n",
    "print(f\"   ‚Ä¢ Average data quality score: {quality_summary['quality_score'].mean():.1f}/100\")\n",
    "print(f\"   ‚Ä¢ Best quality dataset: {quality_summary.loc[quality_summary['quality_score'].idxmax(), 'dataset']}\")\n",
    "\n",
    "print(f\"\\nüîç DATA CHALLENGES IDENTIFIED:\")\n",
    "total_missing_cols = quality_summary['high_missing_cols'].sum()\n",
    "total_duplicates = quality_summary['duplicate_rows'].sum()\n",
    "\n",
    "if total_missing_cols > 0:\n",
    "    print(f\"   ‚Ä¢ High missing data: {total_missing_cols} columns with >50% missing values\")\n",
    "if total_duplicates > 0:\n",
    "    print(f\"   ‚Ä¢ Duplicate records: {total_duplicates:,} duplicate rows found\")\n",
    "\n",
    "print(f\"\\nüí° MACHINE LEARNING STRATEGY:\")\n",
    "print(f\"   1. Data Preprocessing:\")\n",
    "print(f\"      ‚Ä¢ Normalize class labels across datasets (CONFIRMED, CANDIDATE, FALSE_POSITIVE)\")\n",
    "print(f\"      ‚Ä¢ Handle missing values using domain-appropriate imputation\")\n",
    "print(f\"      ‚Ä¢ Remove or impute high-missing columns (>50% missing)\")\n",
    "print(f\"      ‚Ä¢ Feature scaling for algorithm compatibility\")\n",
    "\n",
    "print(f\"\\n   2. Feature Engineering:\")\n",
    "print(f\"      ‚Ä¢ Focus on common astrophysical parameters (period, radius, temperature)\")\n",
    "print(f\"      ‚Ä¢ Create derived features from transit measurements\")\n",
    "print(f\"      ‚Ä¢ Address high correlation pairs to reduce multicollinearity\")\n",
    "\n",
    "print(f\"\\n   3. Model Selection:\")\n",
    "print(f\"      ‚Ä¢ Baseline: Logistic Regression for interpretability\")\n",
    "print(f\"      ‚Ä¢ Advanced: Random Forest/XGBoost for complex patterns\")\n",
    "print(f\"      ‚Ä¢ Evaluation: Precision, Recall, F1-score, AUC-ROC\")\n",
    "print(f\"      ‚Ä¢ Cross-validation for robust performance assessment\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS FOR IMPLEMENTATION:\")\n",
    "print(f\"   ‚úÖ EDA Complete - Comprehensive analysis finished\")\n",
    "print(f\"   üìã Create preprocessing pipeline (src/preprocess.py)\")\n",
    "print(f\"   ü§ñ Implement model training (src/train.py)\")\n",
    "print(f\"   üéØ Build prediction interface (src/predict.py)\")\n",
    "print(f\"   üåê Develop Streamlit web app (app.py)\")\n",
    "print(f\"   üìñ Update documentation (README.md)\")\n",
    "\n",
    "print(f\"\\nüèÜ SUCCESS METRICS:\")\n",
    "print(f\"   ‚Ä¢ Model Accuracy: Target >85% on test set\")\n",
    "print(f\"   ‚Ä¢ Class Balance: Handle imbalanced classes effectively\") \n",
    "print(f\"   ‚Ä¢ Generalization: Consistent performance across datasets\")\n",
    "print(f\"   ‚Ä¢ User Experience: Intuitive web interface for planet hunters\")\n",
    "\n",
    "print(f\"\\n‚≠ê Ready to build the complete exoplanet classification solution!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
