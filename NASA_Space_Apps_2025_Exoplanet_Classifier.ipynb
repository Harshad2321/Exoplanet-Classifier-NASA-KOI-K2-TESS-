{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17450249",
   "metadata": {},
   "source": [
    "# 🚀 NASA Space Apps Challenge 2025 - Exoplanet Classifier\n",
    "## A World Away: Hunting for Exoplanets with AI\n",
    "\n",
    "### Project Overview\n",
    "This notebook provides a comprehensive solution for classifying exoplanets using NASA's KOI (Kepler Objects of Interest), K2, and TESS (Transiting Exoplanet Survey Satellite) datasets. Our goal is to create a high-performance, GPU-accelerated machine learning system capable of identifying confirmed exoplanets, candidates, and false positives.\n",
    "\n",
    "### Challenge Goals\n",
    "- ✅ **High Accuracy**: Achieve >68% classification accuracy\n",
    "- ✅ **GPU Optimization**: Leverage RTX 4060 for accelerated training\n",
    "- ✅ **Real-time Predictions**: Single input and batch processing\n",
    "- ✅ **Explainable AI**: SHAP analysis for model interpretability\n",
    "- ✅ **Production Ready**: Streamlit app with standardized API\n",
    "- ✅ **Memory Optimized**: Efficient data processing pipeline\n",
    "\n",
    "### Dataset Information\n",
    "- **KOI Dataset**: Kepler Objects of Interest with stellar and planetary parameters\n",
    "- **K2 Dataset**: Extended Kepler mission observations\n",
    "- **TESS Dataset**: Transiting Exoplanet Survey Satellite discoveries\n",
    "- **Total Samples**: 21,000+ labeled examples\n",
    "- **Classes**: CONFIRMED, CANDIDATE, FALSE_POSITIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8c0b74",
   "metadata": {},
   "source": [
    "## 🔧 Section 1: Environment Setup and GPU Configuration\n",
    "\n",
    "Setting up the optimal environment for exoplanet classification with GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30be886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core Data Science Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# GPU and Deep Learning Libraries\n",
    "import tensorflow as tf\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    PYTORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"⚠️ PyTorch not available\")\n",
    "\n",
    "# Explainability and Visualization\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"⚠️ SHAP not available - install with: pip install shap\")\n",
    "\n",
    "# Memory optimization\n",
    "import gc\n",
    "import psutil\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"🚀 NASA Space Apps 2025 - Exoplanet Classifier\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Memory Available: {psutil.virtual_memory().available / (1024**3):.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d7077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Configuration and Detection\n",
    "def configure_gpu():\n",
    "    \"\"\"Configure GPU settings for optimal performance\"\"\"\n",
    "    print(\"🔍 GPU Configuration Status\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # TensorFlow GPU Configuration\n",
    "    print(\"📊 TensorFlow GPU Status:\")\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"✅ Found {len(gpus)} GPU(s)\")\n",
    "            for i, gpu in enumerate(gpus):\n",
    "                print(f\"   GPU {i}: {gpu}\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"⚠️ GPU configuration error: {e}\")\n",
    "    else:\n",
    "        print(\"❌ No TensorFlow GPUs found\")\n",
    "    \n",
    "    # PyTorch GPU Configuration\n",
    "    if PYTORCH_AVAILABLE:\n",
    "        print(f\"\\n📊 PyTorch GPU Status:\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"✅ CUDA Available: {torch.cuda.device_count()} device(s)\")\n",
    "            print(f\"   Current Device: {torch.cuda.get_device_name()}\")\n",
    "            print(f\"   Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "            \n",
    "            # Set device\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            print(f\"   Using Device: {device}\")\n",
    "            return device\n",
    "        else:\n",
    "            print(\"❌ CUDA not available for PyTorch\")\n",
    "            return torch.device('cpu')\n",
    "    else:\n",
    "        print(\"❌ PyTorch not installed\")\n",
    "        return None\n",
    "\n",
    "# Configure GPU\n",
    "device = configure_gpu()\n",
    "\n",
    "# Memory optimization function\n",
    "def optimize_memory():\n",
    "    \"\"\"Optimize memory usage\"\"\"\n",
    "    gc.collect()\n",
    "    if PYTORCH_AVAILABLE and torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    memory_percent = psutil.virtual_memory().percent\n",
    "    print(f\"💾 Memory Usage: {memory_percent:.1f}%\")\n",
    "\n",
    "optimize_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d804649b",
   "metadata": {},
   "source": [
    "## 📊 Section 2: Data Loading and Exploration\n",
    "\n",
    "Loading and exploring the NASA exoplanet datasets with memory optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288142e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-Optimized Data Loading\n",
    "class OptimizedDataLoader:\n",
    "    \"\"\"Memory-efficient data loader for NASA exoplanet datasets\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir=\"data/raw/\"):\n",
    "        self.data_dir = data_dir\n",
    "        self.datasets = {}\n",
    "        self.combined_data = None\n",
    "        \n",
    "    def load_dataset(self, filename, dataset_name):\n",
    "        \"\"\"Load individual dataset with memory optimization\"\"\"\n",
    "        filepath = os.path.join(self.data_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"❌ File not found: {filepath}\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"📥 Loading {dataset_name}...\")\n",
    "        \n",
    "        # Load with optimized dtypes\n",
    "        df = pd.read_csv(filepath, low_memory=False)\n",
    "        \n",
    "        # Memory optimization\n",
    "        original_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        \n",
    "        # Optimize numeric columns\n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            \n",
    "            if str(df[col].dtype).startswith('int'):\n",
    "                if col_min > np.iinfo(np.int8).min and col_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif col_min > np.iinfo(np.int16).min and col_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif col_min > np.iinfo(np.int32).min and col_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                if col_min > np.finfo(np.float32).min and col_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "        \n",
    "        # Optimize string columns\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "            num_unique_values = len(df[col].unique())\n",
    "            num_total_values = len(df[col])\n",
    "            if num_unique_values / num_total_values < 0.5:\n",
    "                df[col] = df[col].astype('category')\n",
    "        \n",
    "        optimized_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        print(f\"   📊 Shape: {df.shape}\")\n",
    "        print(f\"   💾 Memory: {original_memory:.1f}MB → {optimized_memory:.1f}MB \"\n",
    "              f\"({((original_memory - optimized_memory) / original_memory * 100):.1f}% reduction)\")\n",
    "        \n",
    "        # Add dataset source\n",
    "        df['dataset_source'] = dataset_name\n",
    "        \n",
    "        self.datasets[dataset_name] = df\n",
    "        return df\n",
    "    \n",
    "    def load_all_datasets(self):\n",
    "        \"\"\"Load all NASA exoplanet datasets\"\"\"\n",
    "        print(\"🚀 Loading NASA Exoplanet Datasets\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Load individual datasets\n",
    "        koi_data = self.load_dataset('koi.csv', 'KOI')\n",
    "        k2_data = self.load_dataset('k2.csv', 'K2')  \n",
    "        tess_data = self.load_dataset('toi.csv', 'TESS')  # Note: TESS data in toi.csv\n",
    "        \n",
    "        return koi_data, k2_data, tess_data\n",
    "\n",
    "# Initialize data loader and load datasets\n",
    "data_loader = OptimizedDataLoader()\n",
    "koi_df, k2_df, tess_df = data_loader.load_all_datasets()\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\n📊 Dataset Summary:\")\n",
    "if koi_df is not None:\n",
    "    print(f\"   KOI Dataset: {koi_df.shape}\")\n",
    "if k2_df is not None:\n",
    "    print(f\"   K2 Dataset: {k2_df.shape}\")\n",
    "if tess_df is not None:\n",
    "    print(f\"   TESS Dataset: {tess_df.shape}\")\n",
    "\n",
    "optimize_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648d1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset structures and find common columns\n",
    "def explore_datasets(datasets):\n",
    "    \"\"\"Explore dataset structures and identify common features\"\"\"\n",
    "    print(\"🔍 Dataset Structure Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    dataset_info = {}\n",
    "    all_columns = set()\n",
    "    \n",
    "    for name, df in datasets.items():\n",
    "        if df is not None:\n",
    "            print(f\"\\n📊 {name} Dataset:\")\n",
    "            print(f\"   Shape: {df.shape}\")\n",
    "            print(f\"   Columns: {df.shape[1]}\")\n",
    "            \n",
    "            # Check for label columns\n",
    "            possible_labels = ['koi_disposition', 'av_training_set', 'disposition', \n",
    "                             'tfopwg_disposition', 'Disposition']\n",
    "            label_col = None\n",
    "            for col in possible_labels:\n",
    "                if col in df.columns:\n",
    "                    label_col = col\n",
    "                    print(f\"   Label Column: {col}\")\n",
    "                    if df[col].dtype == 'object':\n",
    "                        print(f\"   Unique Labels: {df[col].unique()}\")\n",
    "                        print(f\"   Label Counts:\\n{df[col].value_counts()}\")\n",
    "                    break\n",
    "            \n",
    "            if label_col is None:\n",
    "                print(\"   ⚠️ No label column found\")\n",
    "            \n",
    "            dataset_info[name] = {\n",
    "                'shape': df.shape,\n",
    "                'columns': list(df.columns),\n",
    "                'label_column': label_col,\n",
    "                'numeric_columns': list(df.select_dtypes(include=[np.number]).columns),\n",
    "                'categorical_columns': list(df.select_dtypes(include=['object', 'category']).columns)\n",
    "            }\n",
    "            \n",
    "            all_columns.update(df.columns)\n",
    "    \n",
    "    # Find common columns across datasets\n",
    "    common_columns = set(dataset_info[list(dataset_info.keys())[0]]['columns'])\n",
    "    for info in dataset_info.values():\n",
    "        common_columns = common_columns.intersection(set(info['columns']))\n",
    "    \n",
    "    print(f\"\\n🔗 Common Columns Across Datasets: {len(common_columns)}\")\n",
    "    print(f\"   {sorted(list(common_columns))}\")\n",
    "    \n",
    "    return dataset_info, common_columns\n",
    "\n",
    "# Analyze loaded datasets\n",
    "dataset_info, common_columns = explore_datasets({\n",
    "    'KOI': koi_df,\n",
    "    'K2': k2_df, \n",
    "    'TESS': tess_df\n",
    "})\n",
    "\n",
    "# Display sample data from each dataset\n",
    "print(\"\\n📋 Sample Data Preview:\")\n",
    "for name, df in [('KOI', koi_df), ('K2', k2_df), ('TESS', tess_df)]:\n",
    "    if df is not None:\n",
    "        print(f\"\\n{name} Dataset Sample:\")\n",
    "        print(df.head(2).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa81b782",
   "metadata": {},
   "source": [
    "## 🔄 Section 3: Data Preprocessing and Harmonization\n",
    "\n",
    "Standardizing and harmonizing data across all three NASA datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5caf9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Data Preprocessing and Harmonization System\n",
    "class NASADataHarmonizer:\n",
    "    \"\"\"Harmonize and preprocess NASA exoplanet datasets\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_mapping = {}\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.processed_data = None\n",
    "        \n",
    "    def harmonize_datasets(self, koi_df, k2_df, tess_df):\n",
    "        \"\"\"Harmonize column names and data types across datasets\"\"\"\n",
    "        print(\"🔄 Harmonizing NASA Datasets\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        harmonized_datasets = []\n",
    "        \n",
    "        # Define standard feature mapping\n",
    "        feature_map = {\n",
    "            'koi_period': 'period',\n",
    "            'koi_prad': 'radius', \n",
    "            'koi_teq': 'temperature',\n",
    "            'koi_insol': 'insolation',\n",
    "            'koi_depth': 'depth',\n",
    "            'ra': 'ra',\n",
    "            'dec': 'dec'\n",
    "        }\n",
    "        \n",
    "        # Process each dataset\n",
    "        for name, df in [('KOI', koi_df), ('K2', k2_df), ('TESS', tess_df)]:\n",
    "            if df is None:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n📊 Processing {name} Dataset...\")\n",
    "            processed_df = df.copy()\n",
    "            \n",
    "            # Standardize column names\n",
    "            for old_name, new_name in feature_map.items():\n",
    "                if old_name in processed_df.columns:\n",
    "                    processed_df = processed_df.rename(columns={old_name: new_name})\n",
    "            \n",
    "            # Handle different label column formats\n",
    "            label_col = None\n",
    "            target_labels = []\n",
    "            \n",
    "            if 'koi_disposition' in processed_df.columns:\n",
    "                label_col = 'koi_disposition'\n",
    "                # Map KOI labels to standard format\n",
    "                label_mapping = {\n",
    "                    'CONFIRMED': 'CONFIRMED',\n",
    "                    'CANDIDATE': 'CANDIDATE', \n",
    "                    'FALSE POSITIVE': 'FALSE_POSITIVE'\n",
    "                }\n",
    "                processed_df['label'] = processed_df[label_col].map(label_mapping)\n",
    "                target_labels = processed_df['label'].dropna().unique()\n",
    "                \n",
    "            elif 'disposition' in processed_df.columns:\n",
    "                label_col = 'disposition'\n",
    "                processed_df['label'] = processed_df[label_col]\n",
    "                target_labels = processed_df['label'].unique()\n",
    "                \n",
    "            elif 'Disposition' in processed_df.columns:\n",
    "                label_col = 'Disposition'\n",
    "                processed_df['label'] = processed_df[label_col]\n",
    "                target_labels = processed_df['label'].unique()\n",
    "            \n",
    "            # Select core features that exist across datasets\n",
    "            core_features = ['period', 'radius', 'temperature', 'insolation', 'depth', 'ra', 'dec']\n",
    "            available_features = [f for f in core_features if f in processed_df.columns]\n",
    "            \n",
    "            if label_col and len(available_features) > 0:\n",
    "                # Keep only essential columns\n",
    "                keep_columns = available_features + ['label', 'dataset_source']\n",
    "                final_df = processed_df[keep_columns].copy()\n",
    "                \n",
    "                # Remove rows with missing labels\n",
    "                final_df = final_df.dropna(subset=['label'])\n",
    "                \n",
    "                print(f\"   ✅ Features: {available_features}\")\n",
    "                print(f\"   ✅ Labels: {target_labels}\")\n",
    "                print(f\"   ✅ Samples: {len(final_df)}\")\n",
    "                \n",
    "                harmonized_datasets.append(final_df)\n",
    "            else:\n",
    "                print(f\"   ⚠️ Insufficient features or no labels found\")\n",
    "        \n",
    "        # Combine all datasets\n",
    "        if harmonized_datasets:\n",
    "            combined_df = pd.concat(harmonized_datasets, ignore_index=True, sort=False)\n",
    "            \n",
    "            # Final cleanup\n",
    "            combined_df = combined_df.dropna()  # Remove any remaining NaN values\n",
    "            \n",
    "            print(f\"\\n🔗 Combined Dataset:\")\n",
    "            print(f\"   Shape: {combined_df.shape}\")\n",
    "            print(f\"   Features: {[col for col in combined_df.columns if col not in ['label', 'dataset_source']]}\")\n",
    "            print(f\"   Labels: {combined_df['label'].unique()}\")\n",
    "            print(f\"   Label Distribution:\\n{combined_df['label'].value_counts()}\")\n",
    "            \n",
    "            self.processed_data = combined_df\n",
    "            return combined_df\n",
    "        else:\n",
    "            print(\"❌ No datasets could be harmonized\")\n",
    "            return None\n",
    "    \n",
    "    def prepare_features_and_labels(self, data):\n",
    "        \"\"\"Prepare features and labels for machine learning\"\"\"\n",
    "        if data is None:\n",
    "            return None, None\n",
    "            \n",
    "        print(\"\\n🎯 Preparing Features and Labels\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        # Separate features and labels\n",
    "        feature_cols = [col for col in data.columns if col not in ['label', 'dataset_source']]\n",
    "        X = data[feature_cols].copy()\n",
    "        y = data['label'].copy()\n",
    "        \n",
    "        # Handle any remaining missing values\n",
    "        X = X.fillna(X.median())\n",
    "        \n",
    "        # Feature scaling\n",
    "        X_scaled = pd.DataFrame(\n",
    "            self.scaler.fit_transform(X),\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Label encoding\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        print(f\"✅ Features shape: {X_scaled.shape}\")\n",
    "        print(f\"✅ Labels shape: {y_encoded.shape}\")\n",
    "        print(f\"✅ Label mapping: {dict(zip(self.label_encoder.classes_, range(len(self.label_encoder.classes_))))}\")\n",
    "        \n",
    "        return X_scaled, y_encoded, feature_cols\n",
    "\n",
    "# Initialize harmonizer and process data\n",
    "harmonizer = NASADataHarmonizer()\n",
    "combined_data = harmonizer.harmonize_datasets(koi_df, k2_df, tess_df)\n",
    "\n",
    "if combined_data is not None:\n",
    "    X, y, feature_names = harmonizer.prepare_features_and_labels(combined_data)\n",
    "    print(f\"\\n🎉 Data Harmonization Complete!\")\n",
    "    print(f\"   Total Samples: {len(X) if X is not None else 0}\")\n",
    "    print(f\"   Features: {len(feature_names) if feature_names else 0}\")\n",
    "else:\n",
    "    print(\"❌ Data harmonization failed\")\n",
    "\n",
    "optimize_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f498c",
   "metadata": {},
   "source": [
    "## 🔬 Section 4: Feature Engineering and Selection\n",
    "\n",
    "Creating advanced features for improved exoplanet classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab39f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Feature Engineering for Exoplanet Classification\n",
    "class ExoplanetFeatureEngineer:\n",
    "    \"\"\"Create physics-based and statistical features for exoplanet classification\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.engineered_features = []\n",
    "    \n",
    "    def create_physics_features(self, df):\n",
    "        \"\"\"Create physics-based features from planetary parameters\"\"\"\n",
    "        print(\"🔬 Creating Physics-Based Features\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        enhanced_df = df.copy()\n",
    "        new_features = []\n",
    "        \n",
    "        # Physics-based features\n",
    "        if all(col in df.columns for col in ['period', 'radius']):\n",
    "            # Orbital velocity (simplified)\n",
    "            enhanced_df['orbital_velocity'] = 2 * np.pi * enhanced_df['radius'] / (enhanced_df['period'] + 1e-8)\n",
    "            new_features.append('orbital_velocity')\n",
    "            \n",
    "            # Planet size ratio\n",
    "            enhanced_df['period_radius_ratio'] = enhanced_df['period'] / (enhanced_df['radius'] + 1e-8)\n",
    "            new_features.append('period_radius_ratio')\n",
    "        \n",
    "        if all(col in df.columns for col in ['temperature', 'insolation']):\n",
    "            # Habitability proxy\n",
    "            enhanced_df['habitability_index'] = enhanced_df['temperature'] / np.sqrt(enhanced_df['insolation'] + 1e-8)\n",
    "            new_features.append('habitability_index')\n",
    "            \n",
    "            # Stellar energy\n",
    "            enhanced_df['stellar_energy'] = enhanced_df['temperature'] * enhanced_df['insolation']\n",
    "            new_features.append('stellar_energy')\n",
    "        \n",
    "        if all(col in df.columns for col in ['depth', 'radius']):\n",
    "            # Transit signal strength\n",
    "            enhanced_df['transit_signal'] = enhanced_df['depth'] * enhanced_df['radius']\n",
    "            new_features.append('transit_signal')\n",
    "        \n",
    "        # Logarithmic transformations for skewed features\n",
    "        skewed_features = ['period', 'radius', 'insolation', 'depth']\n",
    "        for feature in skewed_features:\n",
    "            if feature in df.columns:\n",
    "                enhanced_df[f'{feature}_log'] = np.log1p(enhanced_df[feature])\n",
    "                new_features.append(f'{feature}_log')\n",
    "        \n",
    "        print(f\"✅ Created {len(new_features)} physics-based features\")\n",
    "        self.engineered_features.extend(new_features)\n",
    "        \n",
    "        return enhanced_df\n",
    "    \n",
    "    def create_statistical_features(self, df):\n",
    "        \"\"\"Create statistical and interaction features\"\"\"\n",
    "        print(\"\\n📊 Creating Statistical Features\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        enhanced_df = df.copy()\n",
    "        new_features = []\n",
    "        \n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        numeric_cols = [col for col in numeric_cols if col not in ['label']]\n",
    "        \n",
    "        # Interaction features (top combinations)\n",
    "        important_pairs = [\n",
    "            ('period', 'radius'),\n",
    "            ('temperature', 'insolation'),\n",
    "            ('radius', 'depth')\n",
    "        ]\n",
    "        \n",
    "        for col1, col2 in important_pairs:\n",
    "            if col1 in numeric_cols and col2 in numeric_cols:\n",
    "                # Product\n",
    "                enhanced_df[f'{col1}_{col2}_product'] = enhanced_df[col1] * enhanced_df[col2]\n",
    "                new_features.append(f'{col1}_{col2}_product')\n",
    "                \n",
    "                # Ratio\n",
    "                enhanced_df[f'{col1}_{col2}_ratio'] = enhanced_df[col1] / (enhanced_df[col2] + 1e-8)\n",
    "                new_features.append(f'{col1}_{col2}_ratio')\n",
    "        \n",
    "        print(f\"✅ Created {len(new_features)} statistical features\")\n",
    "        self.engineered_features.extend(new_features)\n",
    "        \n",
    "        return enhanced_df\n",
    "\n",
    "# Apply feature engineering if we have data\n",
    "if X is not None and combined_data is not None:\n",
    "    print(\"🔧 Engineering Advanced Features\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    feature_engineer = ExoplanetFeatureEngineer()\n",
    "    \n",
    "    # Start with original features\n",
    "    enhanced_data = combined_data.copy()\n",
    "    \n",
    "    # Apply feature engineering\n",
    "    enhanced_data = feature_engineer.create_physics_features(enhanced_data)\n",
    "    enhanced_data = feature_engineer.create_statistical_features(enhanced_data)\n",
    "    \n",
    "    # Prepare final features\n",
    "    feature_cols = [col for col in enhanced_data.columns if col not in ['label', 'dataset_source']]\n",
    "    X_enhanced = enhanced_data[feature_cols].copy()\n",
    "    \n",
    "    # Handle any new missing values\n",
    "    X_enhanced = X_enhanced.fillna(X_enhanced.median())\n",
    "    \n",
    "    # Scale the enhanced features\n",
    "    scaler_enhanced = StandardScaler()\n",
    "    X_final = pd.DataFrame(\n",
    "        scaler_enhanced.fit_transform(X_enhanced),\n",
    "        columns=X_enhanced.columns\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎯 Final Feature Set:\")\n",
    "    print(f\"   Original Features: {len([f for f in feature_names if f in X_enhanced.columns])}\")\n",
    "    print(f\"   Engineered Features: {len(feature_engineer.engineered_features)}\")\n",
    "    print(f\"   Total Features: {X_final.shape[1]}\")\n",
    "    print(f\"   Samples: {X_final.shape[0]}\")\n",
    "    \n",
    "    # Store the scaler for later use\n",
    "    harmonizer.scaler_enhanced = scaler_enhanced\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No data available for feature engineering\")\n",
    "    X_final = X\n",
    "    \n",
    "optimize_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997fe944",
   "metadata": {},
   "source": [
    "## 🚀 Section 5: GPU-Accelerated Model Training\n",
    "\n",
    "Advanced ensemble and neural network training optimized for RTX 4060 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be6976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced GPU-Optimized Model Training System\n",
    "class ExoplanetModelTrainer:\n",
    "    \"\"\"Comprehensive model training system optimized for exoplanet classification\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.model_scores = {}\n",
    "        self.training_history = {}\n",
    "        self.device = device\n",
    "    \n",
    "    def train_gradient_boosting_models(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"Train optimized gradient boosting models\"\"\"\n",
    "        print(\"🌟 Training Gradient Boosting Models\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        # XGBoost with GPU acceleration (if available)\n",
    "        xgb_params = {\n",
    "            'n_estimators': 500,\n",
    "            'max_depth': 8,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'eval_metric': 'logloss'\n",
    "        }\n",
    "        \n",
    "        # Add GPU support if available\n",
    "        if torch.cuda.is_available():\n",
    "            xgb_params['tree_method'] = 'gpu_hist'\n",
    "            xgb_params['gpu_id'] = 0\n",
    "            print(\"🚀 Using GPU acceleration for XGBoost\")\n",
    "        \n",
    "        xgb_model = XGBClassifier(**xgb_params)\n",
    "        xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "        \n",
    "        xgb_pred = xgb_model.predict(X_test)\n",
    "        xgb_score = accuracy_score(y_test, xgb_pred)\n",
    "        \n",
    "        self.models['XGBoost'] = xgb_model\n",
    "        self.model_scores['XGBoost'] = xgb_score\n",
    "        print(f\"   XGBoost Accuracy: {xgb_score:.4f}\")\n",
    "        \n",
    "        # LightGBM with GPU acceleration\n",
    "        lgb_params = {\n",
    "            'n_estimators': 500,\n",
    "            'max_depth': 8,\n",
    "            'learning_rate': 0.1,\n",
    "            'num_leaves': 31,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                lgb_params['device'] = 'gpu'\n",
    "                lgb_params['gpu_platform_id'] = 0\n",
    "                lgb_params['gpu_device_id'] = 0\n",
    "                print(\"🚀 Using GPU acceleration for LightGBM\")\n",
    "            except:\n",
    "                print(\"⚠️ GPU not available for LightGBM, using CPU\")\n",
    "        \n",
    "        lgb_model = LGBMClassifier(**lgb_params)\n",
    "        lgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='logloss')\n",
    "        \n",
    "        lgb_pred = lgb_model.predict(X_test)\n",
    "        lgb_score = accuracy_score(y_test, lgb_pred)\n",
    "        \n",
    "        self.models['LightGBM'] = lgb_model\n",
    "        self.model_scores['LightGBM'] = lgb_score\n",
    "        print(f\"   LightGBM Accuracy: {lgb_score:.4f}\")\n",
    "        \n",
    "        # CatBoost with GPU acceleration\n",
    "        cat_params = {\n",
    "            'iterations': 500,\n",
    "            'depth': 8,\n",
    "            'learning_rate': 0.1,\n",
    "            'l2_leaf_reg': 3,\n",
    "            'random_seed': 42,\n",
    "            'verbose': False,\n",
    "            'allow_writing_files': False\n",
    "        }\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            cat_params['task_type'] = 'GPU'\n",
    "            cat_params['devices'] = '0'\n",
    "            print(\"🚀 Using GPU acceleration for CatBoost\")\n",
    "        \n",
    "        try:\n",
    "            cat_model = CatBoostClassifier(**cat_params)\n",
    "            cat_model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=False)\n",
    "            \n",
    "            cat_pred = cat_model.predict(X_test)\n",
    "            cat_score = accuracy_score(y_test, cat_pred)\n",
    "            \n",
    "            self.models['CatBoost'] = cat_model\n",
    "            self.model_scores['CatBoost'] = cat_score\n",
    "            print(f\"   CatBoost Accuracy: {cat_score:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   CatBoost training failed: {str(e)}\")\n",
    "    \n",
    "    def train_neural_networks(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"Train optimized neural networks with TensorFlow/Keras\"\"\"\n",
    "        print(\"\\n🧠 Training Neural Networks\")\n",
    "        print(\"=\" * 25)\n",
    "        \n",
    "        # Convert to TensorFlow format\n",
    "        X_train_tf = tf.constant(X_train.values.astype(np.float32))\n",
    "        X_test_tf = tf.constant(X_test.values.astype(np.float32))\n",
    "        y_train_tf = tf.constant(y_train.astype(np.int32))\n",
    "        y_test_tf = tf.constant(y_test.astype(np.int32))\n",
    "        \n",
    "        # Advanced Neural Architecture\n",
    "        with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "            nn_model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(256, activation='relu'),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Dense(128, activation='relu'),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(32, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.1),\n",
    "                tf.keras.layers.Dense(len(np.unique(y_train)), activation='softmax')\n",
    "            ])\n",
    "            \n",
    "            # Advanced optimizer and callbacks\n",
    "            optimizer = tf.keras.optimizers.Adam(\n",
    "                learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=0.001,\n",
    "                    decay_steps=100,\n",
    "                    decay_rate=0.96\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            nn_model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            # Training callbacks\n",
    "            callbacks = [\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_accuracy',\n",
    "                    patience=20,\n",
    "                    restore_best_weights=True\n",
    "                ),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss',\n",
    "                    factor=0.5,\n",
    "                    patience=10,\n",
    "                    min_lr=1e-7\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            # Train the model\n",
    "            history = nn_model.fit(\n",
    "                X_train_tf, y_train_tf,\n",
    "                validation_data=(X_test_tf, y_test_tf),\n",
    "                epochs=200,\n",
    "                batch_size=128,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Evaluate\n",
    "            nn_pred = np.argmax(nn_model.predict(X_test_tf, verbose=0), axis=1)\n",
    "            nn_score = accuracy_score(y_test, nn_pred)\n",
    "            \n",
    "            self.models['Neural_Network'] = nn_model\n",
    "            self.model_scores['Neural_Network'] = nn_score\n",
    "            self.training_history['Neural_Network'] = history\n",
    "            print(f\"   Neural Network Accuracy: {nn_score:.4f}\")\n",
    "    \n",
    "    def train_ensemble_models(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"Train ensemble models for maximum performance\"\"\"\n",
    "        print(\"\\n🎯 Training Ensemble Models\")\n",
    "        print(\"=\" * 25)\n",
    "        \n",
    "        # Voting Classifier\n",
    "        if len(self.models) >= 2:\n",
    "            voting_models = [(name, model) for name, model in self.models.items() \n",
    "                           if name != 'Neural_Network']  # Exclude NN for voting\n",
    "            \n",
    "            if voting_models:\n",
    "                voting_clf = VotingClassifier(estimators=voting_models, voting='hard')\n",
    "                voting_clf.fit(X_train, y_train)\n",
    "                \n",
    "                voting_pred = voting_clf.predict(X_test)\n",
    "                voting_score = accuracy_score(y_test, voting_pred)\n",
    "                \n",
    "                self.models['Voting_Ensemble'] = voting_clf\n",
    "                self.model_scores['Voting_Ensemble'] = voting_score\n",
    "                print(f\"   Voting Ensemble Accuracy: {voting_score:.4f}\")\n",
    "        \n",
    "        # Random Forest (as additional ensemble member)\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        \n",
    "        rf_pred = rf_model.predict(X_test)\n",
    "        rf_score = accuracy_score(y_test, rf_pred)\n",
    "        \n",
    "        self.models['Random_Forest'] = rf_model\n",
    "        self.model_scores['Random_Forest'] = rf_score\n",
    "        print(f\"   Random Forest Accuracy: {rf_score:.4f}\")\n",
    "    \n",
    "    def get_best_model(self):\n",
    "        \"\"\"Get the best performing model\"\"\"\n",
    "        if not self.model_scores:\n",
    "            return None, 0\n",
    "        \n",
    "        best_model_name = max(self.model_scores, key=self.model_scores.get)\n",
    "        best_score = self.model_scores[best_model_name]\n",
    "        best_model = self.models[best_model_name]\n",
    "        \n",
    "        return best_model, best_score, best_model_name\n",
    "\n",
    "# Initialize and train models if we have data\n",
    "if X_final is not None and y is not None:\n",
    "    print(\"🎯 Starting Comprehensive Model Training\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_final, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = ExoplanetModelTrainer()\n",
    "    \n",
    "    # Train all models\n",
    "    trainer.train_gradient_boosting_models(X_train, X_test, y_train, y_test)\n",
    "    trainer.train_neural_networks(X_train, X_test, y_train, y_test)\n",
    "    trainer.train_ensemble_models(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Get best model\n",
    "    best_model, best_score, best_model_name = trainer.get_best_model()\n",
    "    \n",
    "    print(f\"\\n🏆 Best Model Results\")\n",
    "    print(\"=\" * 20)\n",
    "    print(f\"Best Model: {best_model_name}\")\n",
    "    print(f\"Best Accuracy: {best_score:.4f}\")\n",
    "    \n",
    "    # Display all model scores\n",
    "    print(f\"\\n📊 All Model Scores:\")\n",
    "    for name, score in sorted(trainer.model_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   {name}: {score:.4f}\")\n",
    "    \n",
    "    # Memory optimization\n",
    "    optimize_memory()\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No data available for model training\")\n",
    "    trainer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7781c2",
   "metadata": {},
   "source": [
    "## 📊 Section 6: Model Evaluation and Explainability\n",
    "\n",
    "Comprehensive evaluation metrics and SHAP explainability for model interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Evaluation and Explainability System\n",
    "class ExoplanetModelEvaluator:\n",
    "    \"\"\"Advanced evaluation and explanation system for exoplanet models\"\"\"\n",
    "    \n",
    "    def __init__(self, trainer, X_test, y_test):\n",
    "        self.trainer = trainer\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.evaluation_results = {}\n",
    "    \n",
    "    def comprehensive_evaluation(self):\n",
    "        \"\"\"Perform comprehensive evaluation of all models\"\"\"\n",
    "        print(\"📊 Comprehensive Model Evaluation\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        for model_name, model in self.trainer.models.items():\n",
    "            print(f\"\\n🔍 Evaluating {model_name}\")\n",
    "            \n",
    "            # Skip neural network for certain operations\n",
    "            if model_name == 'Neural_Network':\n",
    "                predictions = np.argmax(model.predict(self.X_test, verbose=0), axis=1)\n",
    "                probabilities = model.predict(self.X_test, verbose=0)\n",
    "            else:\n",
    "                predictions = model.predict(self.X_test)\n",
    "                probabilities = model.predict_proba(self.X_test) if hasattr(model, 'predict_proba') else None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(self.y_test, predictions)\n",
    "            precision = precision_score(self.y_test, predictions, average='weighted', zero_division=0)\n",
    "            recall = recall_score(self.y_test, predictions, average='weighted', zero_division=0)\n",
    "            f1 = f1_score(self.y_test, predictions, average='weighted', zero_division=0)\n",
    "            \n",
    "            # Store results\n",
    "            self.evaluation_results[model_name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'predictions': predictions,\n",
    "                'probabilities': probabilities\n",
    "            }\n",
    "            \n",
    "            print(f\"   Accuracy:  {accuracy:.4f}\")\n",
    "            print(f\"   Precision: {precision:.4f}\")\n",
    "            print(f\"   Recall:    {recall:.4f}\")\n",
    "            print(f\"   F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    def plot_confusion_matrices(self):\n",
    "        \"\"\"Create confusion matrices for all models\"\"\"\n",
    "        print(f\"\\n📈 Creating Confusion Matrices\")\n",
    "        \n",
    "        n_models = len(self.evaluation_results)\n",
    "        if n_models == 0:\n",
    "            print(\"   No models to evaluate\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten() if n_models > 1 else [axes]\n",
    "        \n",
    "        for idx, (model_name, results) in enumerate(self.evaluation_results.items()):\n",
    "            if idx >= len(axes):\n",
    "                break\n",
    "                \n",
    "            cm = confusion_matrix(self.y_test, results['predictions'])\n",
    "            \n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
    "            axes[idx].set_title(f'{model_name}\\nAccuracy: {results[\"accuracy\"]:.4f}')\n",
    "            axes[idx].set_xlabel('Predicted')\n",
    "            axes[idx].set_ylabel('Actual')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for idx in range(n_models, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def feature_importance_analysis(self):\n",
    "        \"\"\"Analyze feature importance for applicable models\"\"\"\n",
    "        print(f\"\\n🔍 Feature Importance Analysis\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        feature_names = self.X_test.columns.tolist()\n",
    "        importance_data = {}\n",
    "        \n",
    "        for model_name, model in self.trainer.models.items():\n",
    "            try:\n",
    "                if hasattr(model, 'feature_importances_'):\n",
    "                    # Tree-based models\n",
    "                    importance_data[model_name] = model.feature_importances_\n",
    "                elif hasattr(model, 'coef_'):\n",
    "                    # Linear models\n",
    "                    importance_data[model_name] = np.abs(model.coef_[0])\n",
    "                elif model_name == 'Voting_Ensemble' and hasattr(model, 'estimators_'):\n",
    "                    # Voting ensemble - average importance from tree-based estimators\n",
    "                    importances = []\n",
    "                    for estimator in model.estimators_:\n",
    "                        if hasattr(estimator, 'feature_importances_'):\n",
    "                            importances.append(estimator.feature_importances_)\n",
    "                    if importances:\n",
    "                        importance_data[model_name] = np.mean(importances, axis=0)\n",
    "            except Exception as e:\n",
    "                print(f\"   Could not extract importance for {model_name}: {str(e)}\")\n",
    "        \n",
    "        if importance_data:\n",
    "            # Create feature importance plot\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            \n",
    "            # Get the best model's importance\n",
    "            if self.trainer.get_best_model()[2] in importance_data:\n",
    "                best_model_name = self.trainer.get_best_model()[2]\n",
    "                importances = importance_data[best_model_name]\n",
    "                \n",
    "                # Sort features by importance\n",
    "                feature_importance_df = pd.DataFrame({\n",
    "                    'feature': feature_names,\n",
    "                    'importance': importances\n",
    "                }).sort_values('importance', ascending=True)\n",
    "                \n",
    "                # Plot top 20 features\n",
    "                top_features = feature_importance_df.tail(20)\n",
    "                \n",
    "                plt.barh(range(len(top_features)), top_features['importance'])\n",
    "                plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "                plt.xlabel('Feature Importance')\n",
    "                plt.title(f'Top 20 Features - {best_model_name}')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"✅ Feature importance plotted for {best_model_name}\")\n",
    "            else:\n",
    "                print(\"   No feature importance available for best model\")\n",
    "        else:\n",
    "            print(\"   No feature importance data available\")\n",
    "    \n",
    "    def shap_explainability(self):\n",
    "        \"\"\"Create SHAP explanations for model predictions\"\"\"\n",
    "        print(f\"\\n🔬 SHAP Explainability Analysis\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        try:\n",
    "            import shap\n",
    "            shap.initjs()\n",
    "            \n",
    "            best_model, _, best_model_name = self.trainer.get_best_model()\n",
    "            \n",
    "            if best_model_name == 'Neural_Network':\n",
    "                print(\"   SHAP analysis not supported for Neural Networks in this implementation\")\n",
    "                return\n",
    "            \n",
    "            # Create SHAP explainer\n",
    "            if best_model_name in ['XGBoost', 'LightGBM', 'Random_Forest']:\n",
    "                explainer = shap.TreeExplainer(best_model)\n",
    "            elif hasattr(best_model, 'predict_proba'):\n",
    "                explainer = shap.Explainer(best_model.predict_proba, self.X_test[:100])\n",
    "            else:\n",
    "                print(f\"   SHAP not supported for {best_model_name}\")\n",
    "                return\n",
    "            \n",
    "            # Calculate SHAP values (using subset for performance)\n",
    "            sample_size = min(100, len(self.X_test))\n",
    "            shap_values = explainer.shap_values(self.X_test.iloc[:sample_size])\n",
    "            \n",
    "            # Summary plot\n",
    "            if isinstance(shap_values, list):\n",
    "                shap_values = shap_values[1]  # Use positive class for binary classification\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.summary_plot(shap_values, self.X_test.iloc[:sample_size], show=False)\n",
    "            plt.title(f'SHAP Summary Plot - {best_model_name}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"✅ SHAP analysis completed for {best_model_name}\")\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"   SHAP not installed. Install with: pip install shap\")\n",
    "        except Exception as e:\n",
    "            print(f\"   SHAP analysis failed: {str(e)}\")\n",
    "    \n",
    "    def create_evaluation_summary(self):\n",
    "        \"\"\"Create comprehensive evaluation summary\"\"\"\n",
    "        print(f\"\\n📋 Evaluation Summary\")\n",
    "        print(\"=\" * 20)\n",
    "        \n",
    "        if not self.evaluation_results:\n",
    "            print(\"   No evaluation results available\")\n",
    "            return\n",
    "        \n",
    "        # Create summary DataFrame\n",
    "        summary_data = []\n",
    "        for model_name, results in self.evaluation_results.items():\n",
    "            summary_data.append({\n",
    "                'Model': model_name,\n",
    "                'Accuracy': results['accuracy'],\n",
    "                'Precision': results['precision'],\n",
    "                'Recall': results['recall'],\n",
    "                'F1-Score': results['f1_score']\n",
    "            })\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df = summary_df.sort_values('Accuracy', ascending=False)\n",
    "        \n",
    "        print(summary_df.to_string(index=False, float_format='{:.4f}'.format))\n",
    "        \n",
    "        return summary_df\n",
    "\n",
    "# Perform comprehensive evaluation if we have trained models\n",
    "if trainer is not None and trainer.models:\n",
    "    print(\"🎯 Starting Comprehensive Model Evaluation\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = ExoplanetModelEvaluator(trainer, X_test, y_test)\n",
    "    \n",
    "    # Run all evaluations\n",
    "    evaluator.comprehensive_evaluation()\n",
    "    evaluator.plot_confusion_matrices()\n",
    "    evaluator.feature_importance_analysis()\n",
    "    evaluator.shap_explainability()\n",
    "    \n",
    "    # Create summary\n",
    "    evaluation_summary = evaluator.create_evaluation_summary()\n",
    "    \n",
    "    # Memory optimization\n",
    "    optimize_memory()\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No trained models available for evaluation\")\n",
    "    evaluator = None\n",
    "    evaluation_summary = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d3069f",
   "metadata": {},
   "source": [
    "## 💾 Section 7: Model Serialization and Deployment\n",
    "\n",
    "Save trained models and create prediction APIs for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9423d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Serialization and Deployment System\n",
    "import joblib\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "class ExoplanetModelDeployment:\n",
    "    \"\"\"Handle model serialization and deployment preparation\"\"\"\n",
    "    \n",
    "    def __init__(self, trainer, harmonizer, feature_engineer=None, scaler=None):\n",
    "        self.trainer = trainer\n",
    "        self.harmonizer = harmonizer\n",
    "        self.feature_engineer = feature_engineer\n",
    "        self.scaler = scaler\n",
    "        self.model_dir = \"models\"\n",
    "        \n",
    "        # Create models directory\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "    \n",
    "    def save_all_models(self):\n",
    "        \"\"\"Save all trained models with metadata\"\"\"\n",
    "        print(\"💾 Saving All Trained Models\")\n",
    "        print(\"=\" * 28)\n",
    "        \n",
    "        if not self.trainer or not self.trainer.models:\n",
    "            print(\"   No models to save\")\n",
    "            return\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        for model_name, model in self.trainer.models.items():\n",
    "            try:\n",
    "                model_path = os.path.join(self.model_dir, f\"{model_name}_{timestamp}\")\n",
    "                \n",
    "                if model_name == 'Neural_Network':\n",
    "                    # Save TensorFlow model\n",
    "                    model.save(f\"{model_path}.h5\")\n",
    "                    print(f\"   ✅ Saved {model_name} to {model_path}.h5\")\n",
    "                else:\n",
    "                    # Save scikit-learn compatible models\n",
    "                    joblib.dump(model, f\"{model_path}.joblib\")\n",
    "                    print(f\"   ✅ Saved {model_name} to {model_path}.joblib\")\n",
    "                \n",
    "                # Save model metadata\n",
    "                metadata = {\n",
    "                    'model_name': model_name,\n",
    "                    'accuracy': self.trainer.model_scores.get(model_name, 0),\n",
    "                    'timestamp': timestamp,\n",
    "                    'model_type': type(model).__name__\n",
    "                }\n",
    "                \n",
    "                with open(f\"{model_path}_metadata.json\", 'w') as f:\n",
    "                    json.dump(metadata, f, indent=2)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Failed to save {model_name}: {str(e)}\")\n",
    "    \n",
    "    def save_best_model(self):\n",
    "        \"\"\"Save the best performing model with preprocessing pipeline\"\"\"\n",
    "        print(f\"\\n🏆 Saving Best Model Pipeline\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        best_model, best_score, best_model_name = self.trainer.get_best_model()\n",
    "        \n",
    "        if best_model is None:\n",
    "            print(\"   No best model available\")\n",
    "            return\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        pipeline_name = f\"exoplanet_classifier_pipeline_{timestamp}\"\n",
    "        \n",
    "        # Create complete pipeline\n",
    "        pipeline_data = {\n",
    "            'model': best_model,\n",
    "            'model_name': best_model_name,\n",
    "            'accuracy': best_score,\n",
    "            'harmonizer': self.harmonizer,\n",
    "            'scaler': self.scaler if hasattr(self, 'scaler') else None,\n",
    "            'feature_names': list(X_final.columns) if X_final is not None else None,\n",
    "            'timestamp': timestamp,\n",
    "            'metadata': {\n",
    "                'dataset_info': {\n",
    "                    'total_samples': len(combined_data) if combined_data is not None else 0,\n",
    "                    'features': X_final.shape[1] if X_final is not None else 0,\n",
    "                    'classes': len(np.unique(y)) if y is not None else 0\n",
    "                },\n",
    "                'training_info': {\n",
    "                    'test_accuracy': best_score,\n",
    "                    'model_type': type(best_model).__name__\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Save complete pipeline\n",
    "            if best_model_name == 'Neural_Network':\n",
    "                # Save neural network separately\n",
    "                best_model.save(os.path.join(self.model_dir, f\"{pipeline_name}_model.h5\"))\n",
    "                pipeline_data['model'] = None  # Remove model from pickle\n",
    "                joblib.dump(pipeline_data, os.path.join(self.model_dir, f\"{pipeline_name}.joblib\"))\n",
    "                print(f\"   ✅ Neural network saved separately as {pipeline_name}_model.h5\")\n",
    "            else:\n",
    "                joblib.dump(pipeline_data, os.path.join(self.model_dir, f\"{pipeline_name}.joblib\"))\n",
    "            \n",
    "            print(f\"   ✅ Best model pipeline saved: {pipeline_name}.joblib\")\n",
    "            print(f\"   📊 Model: {best_model_name}\")\n",
    "            print(f\"   🎯 Accuracy: {best_score:.4f}\")\n",
    "            \n",
    "            return pipeline_name\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Failed to save pipeline: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def create_prediction_interface(self):\n",
    "        \"\"\"Create a prediction interface class\"\"\"\n",
    "        print(f\"\\n🔧 Creating Prediction Interface\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        interface_code = '''\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "class ExoplanetPredictor:\n",
    "    \"\"\"Production-ready exoplanet classification interface\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline_path: str, neural_model_path: str = None):\n",
    "        \"\"\"Initialize the predictor with saved pipeline\"\"\"\n",
    "        self.pipeline = joblib.load(pipeline_path)\n",
    "        self.model = self.pipeline['model']\n",
    "        self.model_name = self.pipeline['model_name']\n",
    "        self.harmonizer = self.pipeline['harmonizer']\n",
    "        self.scaler = self.pipeline['scaler']\n",
    "        self.feature_names = self.pipeline['feature_names']\n",
    "        \n",
    "        # Load neural network separately if needed\n",
    "        if neural_model_path and self.model_name == 'Neural_Network':\n",
    "            self.model = tf.keras.models.load_model(neural_model_path)\n",
    "    \n",
    "    def predict_single(self, sample_data: Dict) -> Dict:\n",
    "        \"\"\"Predict a single exoplanet sample\"\"\"\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame([sample_data])\n",
    "        \n",
    "        # Apply harmonization and preprocessing\n",
    "        processed_df = self.harmonizer.harmonize_datasets(df, 'user_input')\n",
    "        \n",
    "        # Feature engineering if available\n",
    "        if hasattr(self.harmonizer, 'feature_engineer'):\n",
    "            processed_df = self.harmonizer.feature_engineer.create_physics_features(processed_df)\n",
    "            processed_df = self.harmonizer.feature_engineer.create_statistical_features(processed_df)\n",
    "        \n",
    "        # Select and scale features\n",
    "        X = processed_df[self.feature_names]\n",
    "        if self.scaler:\n",
    "            X = pd.DataFrame(\n",
    "                self.scaler.transform(X),\n",
    "                columns=X.columns\n",
    "            )\n",
    "        \n",
    "        # Make prediction\n",
    "        if self.model_name == 'Neural_Network':\n",
    "            probabilities = self.model.predict(X.values, verbose=0)[0]\n",
    "            prediction = np.argmax(probabilities)\n",
    "        else:\n",
    "            prediction = self.model.predict(X)[0]\n",
    "            probabilities = self.model.predict_proba(X)[0] if hasattr(self.model, 'predict_proba') else None\n",
    "        \n",
    "        return {\n",
    "            'prediction': int(prediction),\n",
    "            'confidence': float(np.max(probabilities)) if probabilities is not None else None,\n",
    "            'probabilities': probabilities.tolist() if probabilities is not None else None,\n",
    "            'model_used': self.model_name\n",
    "        }\n",
    "    \n",
    "    def predict_batch(self, samples: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Predict multiple samples\"\"\"\n",
    "        return [self.predict_single(sample) for sample in samples]\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        \"\"\"Get model information\"\"\"\n",
    "        return {\n",
    "            'model_name': self.model_name,\n",
    "            'accuracy': self.pipeline['accuracy'],\n",
    "            'features_count': len(self.feature_names),\n",
    "            'timestamp': self.pipeline['timestamp'],\n",
    "            'metadata': self.pipeline['metadata']\n",
    "        }\n",
    "\n",
    "# Usage example:\n",
    "# predictor = ExoplanetPredictor('path_to_pipeline.joblib')\n",
    "# result = predictor.predict_single({'period': 3.5, 'radius': 1.2, 'temperature': 5800, ...})\n",
    "        '''\n",
    "        \n",
    "        # Save the interface code\n",
    "        with open('exoplanet_predictor.py', 'w') as f:\n",
    "            f.write(interface_code)\n",
    "        \n",
    "        print(\"   ✅ Prediction interface created: exoplanet_predictor.py\")\n",
    "        \n",
    "        return interface_code\n",
    "\n",
    "# Execute deployment if we have trained models\n",
    "if trainer is not None and trainer.models:\n",
    "    print(\"🚀 Preparing Model Deployment\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Initialize deployment system\n",
    "    deployment = ExoplanetModelDeployment(\n",
    "        trainer=trainer,\n",
    "        harmonizer=harmonizer,\n",
    "        scaler=scaler_enhanced if 'scaler_enhanced' in locals() else None\n",
    "    )\n",
    "    \n",
    "    # Save all models\n",
    "    deployment.save_all_models()\n",
    "    \n",
    "    # Save best model pipeline\n",
    "    pipeline_name = deployment.save_best_model()\n",
    "    \n",
    "    # Create prediction interface\n",
    "    prediction_interface = deployment.create_prediction_interface()\n",
    "    \n",
    "    print(f\"\\n✅ Deployment preparation completed!\")\n",
    "    print(f\"   📁 Models saved in: {deployment.model_dir}/\")\n",
    "    print(f\"   🏆 Best pipeline: {pipeline_name}\")\n",
    "    print(f\"   🔧 Prediction interface: exoplanet_predictor.py\")\n",
    "    \n",
    "    # Memory optimization\n",
    "    optimize_memory()\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No trained models available for deployment\")\n",
    "    deployment = None"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
